{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b93153",
   "metadata": {},
   "source": [
    "# Excercise 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938f9f1",
   "metadata": {},
   "source": [
    "Based on https://python.langchain.com/docs/expression_language/get_started but with use of a LLM model from the disc.\n",
    "\n",
    "To get the model:\n",
    "- create `models` folder in the main `Rag-experiments` directory\n",
    "- download `mistral-7b-instruct-v0.1.Q4_0.gguf` from https://gpt4all.io/index.html and save it in `models`\n",
    "\n",
    "\n",
    "TODO:\n",
    "1. Change temperature of the model. What can you observe?\n",
    "2. Look for another model from GPT4ALL, download it and plug instead of mistral-7b-instruct. What can you observe?\n",
    "3. Play with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54a15266-c5d9-4dc2-9a5f-872310ecf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7274972-fc5e-4e2d-ab09-b3aa53071498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "llm = GPT4All(  model=\"../models/mistral-7b-instruct-v0.1.Q4_0.gguf\",\n",
    "                backend=\"gptj\",\n",
    "                verbose=True,\n",
    "                temp=0.3,\n",
    "                top_p=0.6,\n",
    "                repeat_penalty=1.18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60f19dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='tell me a short joke about {topic}'))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "\n",
    "# Initialize output parser \n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d023b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chain\n",
    "\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26f9b080-0810-43fb-b39e-6b8ba30e8ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Chicken: Why did the chicken cross the road? To prove to the opossum it could be done.\n"
     ]
    }
   ],
   "source": [
    "answer = chain.invoke({\"topic\":\"chicken\"})\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
